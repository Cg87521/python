## 卷积神经网络

上周欠了一篇多线程爬虫，不是我偷懒，而是我跑去搞大新闻去了。由于工作需要，我中途去研究了一些别的东西。对，本周要讲的就是卷积神经网络（Convolutional Neural Network,**CNN**）。也是目前在计算机视觉领域的不二选择。俺们今天就是要看看这个名字有点唬人的算法到底是怎么回事。不过在上主菜之前，我们总要上点小菜铺垫一下。

### 预期的收获



## 神经网络

这里的神经网络（ Neural Network）指的不是一般普通青年认为的生物神经网络，而是人工神经网络。它是一种模仿动物神经网络行为特征，进行分布式并行信息处理的算法数学模型。说白了就是一个可以逼近任意函数的机制。



## BP神经网络

在讲卷积神经网络之前我们不得不先介绍一下BP(back propagation)神经网络，在我看来它的思想是目前各种巨多层神经网络都含有的，所以百度百科上说它是目前应用最广泛的神经网络。用一句话来介绍它就是：**一种按照误差逆向传播算法训练的多层前馈神经网络 **。

如果说感知机（单层神经网络，[感知机](http://www.hankcs.com/ml/the-perceptron.html)）是神经网络发展的最早转折点，那么BP神经网络的出现就解决了多层神经网络隐含层中的连接权重的学习问题，并在数学上给出了完整推导。

有关于BP神经网络的细节描述我就不在这里细说了（因为我还要做分享PPT，真的忙不过来...）。我就按自己的理解来概括一下BP神经网络，关于具体的细节请参考：[如何理解神经网络里面的反向传播算法？](https://www.zhihu.com/question/24827633)

**神经网络就是多个神经元的连接，上一级神经元的输出是上一个神经元的输入，而数据在从两层神经元之间传播的时候需要乘上这连个神经元的对应权值。最后，在输出层得到模型训练的结果，与真实数据比较，根据定义一个误差函数，它反映了输出层的结果与真实结果之间的误差大小。重点来了，首先我们要知道在模型中需要训练的参数就是层与层之间的连接权重，那么在得到了误差函数之后，怎么使最小化差函数来得到最优的模型了。总结起来就是：1.反向传播，即对损失函数和权重值利用链式法则求导（因为每个权值参数都可以看做是损失函数的参数）；2.梯度下降（梯度反方向即函数值下降最快的方向），即使权重向误差最小化的方向逼近。根据以上两点对权重值进行更新直至误差函数最小，模型训练完成。**

## 卷积神经网络



好了，没有在BP神经网络上花太多口水，我们直接进入对卷积神经网络的学习，来看看为什么它在图像识别领域有这么的名气。首先对它下个定义：

**卷积神经网络（Convolutional Neural Network）是一种前馈神经网络，它的人工神经元可以响应一部分覆盖范围内的周围单元，对于大型图像处理有出色表现。**

卷积神经网络在结构上可以分为：输入层，卷积层，池化层，全连接层。

下面这张图很形象的反映了结构：

![1](http://or0zpjxns.bkt.clouddn.com/1.png)

还是用我自己的话来描述一下卷积神经网络的工作方式：**首先对于一个待识别的图像而言，输入层将其转化为像素矩阵作为输入数据。通过卷积层来提取图像中的特征，再通过池化层对得到的特征矩阵进行降维，并且保持了图像的旋转不变性，根据需求选择再次进行卷积操作以及池化，在经过卷积层，池化层的交替进行后，将最后得到的特征作为输入到全连接层。全连接层可以理解为最后的分类器，输出最后的分类结果，一般可以选择BP网络，也可以选择如SVM等其他分类器。其中我们训练模型的思路在介绍BP神经网络的时候已经讲过了（反向传播+最小化误差函数），只不过我们需要训练的参数变成了卷积层中的权重矩阵或者说是卷积核，过滤器都行。这些参数是用来提取图像特征的，它们的不断更新使得图像的特征越来越准确。**

![v2-cf87890eb8f2358f23a1ac78eb764257_r](http://or0zpjxns.bkt.clouddn.com/v2-cf87890eb8f2358f23a1ac78eb764257_r.png)

如果你从没有了解过CNN，那么我相信你在看完了上面那段话肯定什么都不懂:)

下面我会分别描述CNN中的各个结构在其中的具体作用，在看了这些之后你再回过头看上面那段话，如果你能看懂说明你对CNN的原理就有了一个基本的认识了。

## 输入层

如果我们有一张JPG格式的320$*$320大小的彩色图片，那么它对应的数据就有320$*$320$*$3个元素,含红(R)、绿(G)、蓝(B)三个颜色通道，其中每个数字的值从0到255不等，其描述了对应那点的像素灰度。计算机没有眼睛，而这个数组就是计算机能够获得的唯一数据输入。

![rgb](http://or0zpjxns.bkt.clouddn.com/rgb.png)

##卷积层

从名字上就能看出来，卷积层应该是整个CNN中最核心的部分。在卷积层中，我们会随机初始化一个或多个卷积核（其实就是一个权重矩阵，一个卷积核提取一个特征，多个卷积核能够提取多个特征），再利用这个卷积核对输入的像素矩阵从左到右，从上到下进行滑动覆盖，最后对对象的像素点做乘法再相加。

图像像素矩阵：

![`1.1`](http://or0zpjxns.bkt.clouddn.com/1.1.png)

卷积核（权重矩阵）长这样：

![1.2](http://or0zpjxns.bkt.clouddn.com/1.2.png)

卷积操作：

![1.3](http://or0zpjxns.bkt.clouddn.com/1.3.jpg)

一次完整的卷积操作示意图如下所示：

![卷积操作](http://or0zpjxns.bkt.clouddn.com/6.gif)

ps：关于卷积操作的完整过程可以参考：[卷积操作示意图](http://cs231n.github.io/convolutional-networks/)

由于卷积核在图像矩阵上面进行滑动，最后得到了一个特征矩阵，所以也可以将它称之为一个过滤器。它保留了图像中一些重要的特征，并且降低了图像的维度。为什么这么说呢？**因为如果将卷积核和图像中的对应区域沿任意方向都拉成一个向量的话，卷积运算其实就是向量的内积运算。而向量内积是衡量两个向量之间的相似性的，所以卷积核在图像某一区域的卷积结构实际上就是这个卷积核与图像该区域相似性的度量。**

需要注意的是，卷积核除了权重矩阵一般还包含一个偏置，对应数据本身的倾向。

决定卷积层里面卷积核的参数数量的数字总共有四个：权重矩阵的个数,num_channel(由卷积层的输入决定，为卷积层的输入的channel数量),kernel_height,kernel_width(决定卷积核的kernel_size)。卷积层还有两个参数：步长（stride）和填充(padding)。

一般卷积神经网络的第一个卷积层的卷积核是用来检测低阶特征，比如边，角，曲线等。而随着卷积层的增加，对应卷积核检测的特征就更加复杂。因为上一卷积层总是下一卷积层的输入，所以是在上一卷积层的基础上再次提取特征，最后提取出越来越复杂的特征。卷积层的重要性在于，它所提取的特征的好坏，决定了分类器结果的准确性，所以在CNN中我们一般设置多个卷积核，初始化不同的卷积核来提取多个**不同**的特征，从而增加我们对图像的描述角度。

就卷积神经网络而言，其中的参数就是连接两层数据之间的桥梁，而卷积只不过是参数的一种组织形式而已。对于图片来说，卷积就像是一个模板在图片上滑动，它通过一小块图片(像素矩阵上的一小块区域)与权重矩阵上的值对应相乘得到下一层的数据，用以提取特征和保存图片的空间特征。
比如输入图像的数组大小是32* 32* 3，3可以看作是图像的深度（即RGB），若卷积层中含一个5x5x3的卷积核(ps:卷积核的深度必须和输入图像的深度相同)。那么通过一个卷积核与输入图像进行卷积操作可以得到一个28x28x1的特征图。

卷积核里每个元素的值通常是随机初始化再用BP算梯度做训练，整个网络的训练就是为了学习卷积核，卷积核其实就是权重矩阵，是我们要学习的参数。

在卷积层中我们要特别说明一下其中的激活函数和权值共享这几个概念：
**权值共享**
权值共享的意思是给定一张输入图片，用初始化的卷积核对图像进行卷积操作，卷积核里的每一个元素都是权重，而卷积核是在这张图上进行从左到右从上到下的滑动，即对于这张图来说，卷积核是共享的，也就是所谓的权重共享
**激励函数**
激励函数的作用就是处理输入产生输出。把我们定义的线性函数的输出转换成我们想要的格式。
以上就是卷积层的原理以及具体作用。

## 池化层

下面我们来介绍一下池化(pooling)层，按照字面意思可能让人摸不着头脑，我们可以简单的将它理解为采样(subsampling)。采用的池化手段一般是最大值池化和平均值池化，池化的操作就是在卷积层输出的特征矩阵上选定一小块区域，比如一个4*4矩阵，如果采用最大值池化，那么就选取16个像素点中最大的那个值，平均值池化类似。

下面是最大值池化的示意图：

![池化](http://or0zpjxns.bkt.clouddn.com/%E6%B1%A0%E5%8C%96.png)



由上图我们可以看出，经过卷积层过滤之后的特征矩阵(4x4)在经过最大值池化操作后生成了一个2x2的特征矩阵，所以池化层最主要的就是作用就是对特征矩阵进行降维（可以理解为对图像的一种模糊策略）。这一点在我们处理较大的图像是非常有用，它可以有效的减少我们需要训练的参数。并且，池化操作增强了图像的旋转不变性，因为无论图像怎么旋转，但是对于取最大值或取平均值而言，池化之后的结构都是不变的。

由于池化层可以对图像数据降维，所以它总是出现在卷积层之后（与卷积层一样，底层的池化层模糊低级的图像特征，如线条，边框等；高层的池化层模糊高级语义特征，如眼睛，耳朵等），即卷积，池化交替进行，在提取特征的同时也在对数据降维。示意图如下：

![卷积池化](http://or0zpjxns.bkt.clouddn.com/v2-8e9d7ec0662e903e475bd93a64067554_r.png)



### 全连接层

在经过多层卷积和池化操作之后，我们最终将要以类的形式输出结果。卷积层和池化层的作用在于提取好的图像特征，并且减少我们需要训练的参数。在全连接层中定义了一个类似分类交叉熵的损失函数，用于衡量分类误差。在数据输入后，完成前向传播，反向传播就向着最小化损失函数的方向开始更新权重与偏差。其优化原理与我们在前面介绍过的BP神经网络类似。

最后再来总结一下：

无论是卷积神经网络还是其他什么神经网络，本质上就是在用一层层简单的函数来逼近一个非常复杂的函数，而这个逼近的过程就是通过一次次向后传播(back propagation)误差来更新参数从而使损失函数最小化。至于输出层前面是一个什么过程（隐含层），取决于我们这个神经网络是用来干嘛的，比如CNN就是在提取图像特征，而输出层本身在接受最终的特征数据进行分类。如果输出层前的隐含层很多很多，即可以认为模型学习过程很长，所以就有了深度学习(Deep Learning)这个说法:)








